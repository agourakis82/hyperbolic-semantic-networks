# GUIA DE WORKFLOW - CLUSTER DARWIN
**Data:** 2025-11-02  
**Status:** Production-Ready

---

## üéØ RESUMO EXECUTIVO: 3 OP√á√ïES DE TRABALHO

### OP√á√ÉO A: MacBook APENAS (RECOMENDADO) ‚≠ê
- **Onde:** MacBook Pro M3 Max (local, com voc√™)
- **Como:** Cursor local + kubectl remoto via Tailscale
- **Vantagem:** Mobilidade total + Starlink no carro
- **Cluster:** Sempre acess√≠vel via Tailscale

### OP√á√ÉO B: Windows RDP (Performance M√°xima)
- **Onde:** Dell 5860 via RDP do MacBook
- **Como:** Cursor no Windows + acesso local WSL
- **Vantagem:** 256GB RAM, RTX 4000 Ada, performance m√°xima
- **Uso:** Desenvolvimento pesado, debugging cluster

### OP√á√ÉO C: H√≠brido (Melhor dos 2 mundos) ‚≠ê‚≠ê‚≠ê
- **Normal:** MacBook - mobilidade
- **Heavy:** RDP Windows - quando precisa de power
- **Switching:** Simples via RDP do MacBook

---

## üì± OP√á√ÉO A: MACBOOK APENAS (RECOMENDADO)

### Setup (J√Å CONFIGURADO ‚úÖ):

```bash
# No MacBook, voc√™ j√° tem:
export KUBECONFIG=~/.kube/config-darwin

# Testar conectividade
kubectl get nodes
kubectl get pods --all-namespaces
```

### Workflow Di√°rio - Como os Agentes AI Trabalham:

1. **Abrir Cursor no MacBook**
   ```bash
   cd ~/workspace/kec-biomaterials-scaffolds
   cursor .
   ```

2. **Agente AI detecta automaticamente:**
   - L√™ `.cursorrules` (regras autom√°ticas)
   - Executa `./.darwin/sync-check.sh`
   - L√™ `SYNC_STATE.json` (estado compartilhado)
   - Verifica cluster dispon√≠vel via kubectl

3. **Agente submete job de treinamento:**
   ```yaml
   # Exemplo: training MicroCT
   apiVersion: batch/v1
   kind: Job
   metadata:
     name: training-microct
     namespace: kec-biomaterials
   spec:
     template:
       spec:
         containers:
         - name: pytorch
           image: pytorch/pytorch:2.0.1-cuda11.8-cudnn8-runtime
           command: ["python", "train.py"]
           resources:
             requests:
               nvidia.com/gpu: 1
               memory: 32Gi
             limits:
               nvidia.com/gpu: 1
               memory: 64Gi
         nodeSelector:
           darwin.dev/gpu: nvidia-l4  # For√ßa rodar no T560!
         restartPolicy: Never
   ```

4. **Agente monitora execu√ß√£o:**
   ```bash
   kubectl logs -f job/training-microct -n kec-biomaterials
   ```

5. **Voc√™ acessa m√©tricas no Grafana:**
   - Browser: http://100.112.110.114:30000
   - Acessa via Tailscale de qualquer lugar!

### Vantagens:
- ‚úÖ Trabalha de qualquer lugar
- ‚úÖ Starlink garante conectividade 24/7
- ‚úÖ Lat√™ncia baixa (~30ms via Tailscale)
- ‚úÖ MacBook M3 Max √© PODEROSO (48GB RAM, 14 cores)
- ‚úÖ Zero depend√™ncia do Windows/WSL

### Desvantagens:
- ‚ö†Ô∏è Sem acesso direto √† GPU local (mas acessa T560 remoto)
- ‚ö†Ô∏è Debugging cluster requer kubectl remoto (funciona bem)

---

## üñ•Ô∏è OP√á√ÉO B: WINDOWS RDP (PERFORMANCE M√ÅXIMA)

### Setup:

```bash
# No MacBook, conectar via RDP
# Microsoft Remote Desktop
# Host: 192.168.3.207 (Dell 5860 IP local)
# Ou via Tailscale: 100.112.110.114

# Dentro do Windows:
# 1. Abrir Cursor (instalado no Windows)
# 2. Abrir WSL terminal
# 3. cd /home/agourakis82/workspace/kec-biomaterials-scaffolds
# 4. cursor . (abre Cursor no c√≥digo)
```

### Workflow Di√°rio:

```bash
# No Windows/WSL (via RDP do MacBook):

# 1. Agentes AI operam com acesso LOCAL ao cluster
kubectl get nodes  # Acesso direto, <1ms latency!

# 2. Submit jobs
kubectl apply -f jobs/training.yaml

# 3. Debugging profundo
kubectl exec -it pod/training-xyz -- bash
# Acesso DIRETO aos containers!
```

### Vantagens:
- ‚úÖ Performance M√ÅXIMA (256GB RAM, RTX 4000 Ada)
- ‚úÖ Acesso local ao cluster (<1ms)
- ‚úÖ Debugging PROFUNDO (kubectl exec, logs, etc)
- ‚úÖ GPU local dispon√≠vel (futuro)

### Desvantagens:
- ‚ö†Ô∏è Precisa estar na mesma rede (ou Tailscale)
- ‚ö†Ô∏è N√£o funciona no carro com Starlink (RDP via Starlink = lag)
- ‚ö†Ô∏è Depend√™ncia do Windows estar ligado

---

## üîÑ OP√á√ÉO C: H√çBRIDO (RECOMENDADO PARA VOC√ä!)

### Estrat√©gia:

**Trabalho Normal (90% do tempo):**
- MacBook local
- Cursor no MacBook
- kubectl remoto via Tailscale
- Submete jobs para T560 (GPU) ou Mac Pro (CPU)
- Mobilidade total

**Desenvolvimento Pesado (10% do tempo):**
- RDP do MacBook ‚Üí Windows Dell 5860
- Cursor no Windows (256GB RAM, RTX local)
- Debugging profundo do cluster
- Desenvolvimento de features complexas

### Switching:

```bash
# OP√á√ÉO 1: MacBook local
cd ~/workspace/kec-biomaterials-scaffolds
cursor .
export KUBECONFIG=~/.kube/config-darwin
kubectl get nodes

# OP√á√ÉO 2: RDP Windows (quando precisar)
# Conecta RDP via Microsoft Remote Desktop
# Abre Cursor no Windows
# WSL terminal: kubectl get nodes (acesso local!)
```

---

## ü§ñ COMO OS AGENTES AI FUNCIONAM

### 1. Agente Abre o Workspace:

```bash
# Cursor detecta .cursorrules AUTOMATICAMENTE
# Agente l√™ instru√ß√µes:
# - Verificar SYNC_STATE.json
# - Rodar ./.darwin/sync-check.sh
# - Ver fase atual, progresso, locks
```

### 2. Agente Identifica Tarefa:

```
Usu√°rio: "Treinar modelo de microCT com uncertainty"

Agente analisa:
  ‚úÖ Package correto: darwin-preprocessing + darwin-uncertainty
  ‚úÖ Cluster dispon√≠vel? Sim (T560 com L4 24GB)
  ‚úÖ Recursos suficientes? Sim (128GB RAM, 32 cores)
  ‚úÖ Queue: kec-biomaterials (prioridade 40%)
```

### 3. Agente Submete Job:

O agente cria o manifest YAML e submete via kubectl:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: training-microct-uncertainty
  namespace: kec-biomaterials
  labels:
    app: darwin
    queue: kec-biomaterials-queue
spec:
  template:
    spec:
      schedulerName: volcano  # Usa Volcano scheduler!
      containers:
      - name: training
        image: pytorch/pytorch:2.0.1-cuda11.8-cudnn8-runtime
        command: ["python", "train_uncertainty.py"]
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: 64Gi
            cpu: 16
          limits:
            nvidia.com/gpu: 1
            memory: 96Gi
            cpu: 24
      nodeSelector:
        darwin.dev/gpu: nvidia-l4  # For√ßa T560!
      restartPolicy: Never
```

### 4. Agente Atualiza Estado:

```json
{
  "last_action": {
    "agent_id": "cursor-agent-abc123",
    "action": "submit_training_job",
    "timestamp": "2025-11-02T13:30:00-03:00",
    "details": "Training microCT uncertainty on T560 L4 GPU"
  },
  "active_jobs": {
    "training-microct-uncertainty": {
      "status": "running",
      "node": "maria",
      "gpu": "nvidia-l4",
      "started_at": "2025-11-02T13:30:05-03:00"
    }
  }
}
```

---

## üîê MULTI-AGENTE: PROTE√á√ïES AUTOM√ÅTICAS

### Cen√°rio 1: Dois Agentes Simultaneamente

```
AGENTE A (MacBook):
  - Abre workspace kec-biomaterials
  - L√™ SYNC_STATE.json
  - V√™: agente_b est√° ativo em "pcs-meta-repo"
  - Decis√£o: OK trabalhar (repos diferentes!)

AGENTE B (Windows RDP):
  - Abre workspace pcs-meta-repo
  - L√™ SYNC_STATE.json
  - V√™: agente_a est√° ativo em "kec-biomaterials"
  - Decis√£o: OK trabalhar (repos diferentes!)

RESULTADO: Zero conflitos! ‚úÖ
```

### Cen√°rio 2: Conflito de Recurso

```
AGENTE A (MacBook):
  - Tenta submeter job com 2 GPUs
  - Kueue verifica: apenas 1 GPU dispon√≠vel (L4 no T560)
  - Job fica PENDING na queue
  - Agente avisa usu√°rio: "Aguardando GPU dispon√≠vel"

AGENTE B (Windows):
  - Tenta submeter job com 1 GPU
  - Kueue verifica: L4 est√° em uso pelo Agente A
  - Op√ß√µes:
    1. Aguardar na queue (autom√°tico)
    2. Usar borrowing (pegar GPU de outro repo)
    3. Usar preemption (se prioridade maior)

RESULTADO: Kueue gerencia automaticamente! ‚úÖ
```

### Cen√°rio 3: Conflito de C√≥digo

```
AGENTE A (MacBook):
  - Editando train_uncertainty.py
  - Cria lock em SYNC_STATE.json:
    "locks": {
      "darwin-uncertainty/train_uncertainty.py": {
        "agent_id": "agent-a",
        "timestamp": "2025-11-02T13:30:00-03:00"
      }
    }

AGENTE B (Windows):
  - Tenta editar train_uncertainty.py
  - L√™ SYNC_STATE.json
  - V√™ lock do Agente A
  - AVISA USU√ÅRIO: "Arquivo locked por outro agente!"
  - Oferece alternativas:
    1. Aguardar lock expirar (30 min)
    2. Trabalhar em outro arquivo
    3. Coordenar com usu√°rio

RESULTADO: Zero sobrescrita acidental! ‚úÖ
```

---

## üìä MONITORAMENTO: GRAFANA + PROMETHEUS

### Acesso:

```bash
# Via Tailscale (de qualquer lugar!)
http://100.112.110.114:30000

# Login primeira vez:
# admin / prom-operator
# Trocar senha depois!
```

### Dashboards Dispon√≠veis:

1. **Kubernetes Cluster Overview**
   - 5 nodes, status, uptime
   - CPU/RAM/Disk por node
   - Pods running/pending/failed

2. **NVIDIA GPU (T560)**
   - GPU utilization (%)
   - Memory used/total (24GB)
   - Temperature
   - Power usage (W)

3. **Node Metrics**
   - CPU usage por core (32 cores T560!)
   - RAM usage (128GB T560)
   - Network I/O
   - Disk I/O

4. **Kueue Metrics**
   - Jobs pending/running/completed por queue
   - Resource quotas (usado vs dispon√≠vel)
   - Borrowing/lending activity

5. **Volcano Metrics**
   - Gang scheduling stats
   - Fair share distribution
   - Job completion time

---

## üéØ RECOMENDA√á√ÉO FINAL PARA VOC√ä

### Workflow Ideal:

**DIA-A-DIA (Casa/Viagens):**
```bash
# MacBook Pro M3 Max (local)
cd ~/workspace/kec-biomaterials-scaffolds
cursor .

# Agentes AI trabalham normalmente:
# - Submetem jobs para T560 (GPU)
# - Acessam NFS do Mac Pro (datasets)
# - Monitoram via Grafana

# Voc√™ trabalha de qualquer lugar:
# - Casa (WiFi Gigabit)
# - Carro (Starlink)
# - Viagens (Starlink)
```

**DESENVOLVIMENTO PESADO (Quando precisar):**
```bash
# RDP do MacBook ‚Üí Windows Dell 5860
# Cursor no Windows (256GB RAM, RTX local)
# Debugging profundo
# Testes com GPU local (futuro)
```

**VANTAGENS:**
- ‚úÖ 90% do tempo: MacBook (mobilidade)
- ‚úÖ 10% do tempo: Windows (performance)
- ‚úÖ Starlink garante acesso 24/7
- ‚úÖ Cluster sempre dispon√≠vel via Tailscale
- ‚úÖ Zero downtime

---

## üîß COMANDOS √öTEIS (CHEAT SHEET)

### Ver Cluster:
```bash
kubectl get nodes -o wide
kubectl top nodes  # CPU/RAM usage
```

### Ver Jobs:
```bash
kubectl get jobs -n kec-biomaterials
kubectl get pods -n kec-biomaterials
kubectl logs -f pod/training-xyz -n kec-biomaterials
```

### Ver Queues (Kueue):
```bash
kubectl get clusterqueues
kubectl get queues -A
kubectl describe clusterqueue kec-biomaterials
```

### Ver Queues (Volcano):
```bash
kubectl get queues.scheduling.volcano.sh
kubectl describe queue kec-biomaterials-queue
```

### Submeter Job:
```bash
kubectl apply -f jobs/my-job.yaml
```

### Cancelar Job:
```bash
kubectl delete job my-job -n kec-biomaterials
```

### Debugging:
```bash
# Entrar em container rodando
kubectl exec -it pod/training-xyz -n kec-biomaterials -- bash

# Ver eventos
kubectl get events -n kec-biomaterials --sort-by='.lastTimestamp'

# Descrever pod (ver por que falhou)
kubectl describe pod training-xyz -n kec-biomaterials
```

---

## üìö DOCUMENTOS IMPORTANTES

**Leitura Obrigat√≥ria:**
1. `.cursorrules` - Regras para agentes AI
2. `ARCHITECTURE.md` - Estrutura do projeto
3. `AGENT_GUIDE.md` - Guia para agentes
4. `SYNC_STATE.json` - Estado atual
5. `.darwin-cluster.yaml` - Config do cluster

**Guias:**
- `COMO_USAR_SYNC.md` - Sistema de sincroniza√ß√£o
- `MACBOOK_QUICKSTART.md` - Atalhos MacBook
- `GRAFANA_ACCESS_GUIDE.md` - Como acessar m√©tricas

---

## üéä RESUMO EXECUTIVO

**VOC√ä TEM UM SUPERCOMPUTADOR PESSOAL:**
- 500GB RAM
- 69 cores CPU
- 4 GPUs (NVIDIA L4 24GB + 3 Metal)
- Multi-arch (x86_64 + ARM64)
- 3TB storage compartilhado
- Acess√≠vel de QUALQUER LUGAR via Starlink + Tailscale

**TRABALHE COMO QUISER:**
- MacBook (mobilidade) OU
- Windows RDP (performance) OU
- H√≠brido (melhor dos 2 mundos)

**AGENTES AI GERENCIAM TUDO:**
- Submetem jobs automaticamente
- Escolhem melhor node (T560 GPU, Mac Pro CPU, etc)
- Monitoram execu√ß√£o
- Salvam resultados
- Zero conflitos entre agentes

**STATUS:** üü¢ PRODUCTION-READY!

