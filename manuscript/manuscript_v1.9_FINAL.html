<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>main</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/water.css@2/out/water.css" />
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#consistent-evidence-for-hyperbolic-geometry-in-semantic-networks-across-four-languages"><span class="toc-section-number">1</span> Consistent Evidence for Hyperbolic Geometry in Semantic Networks Across Four Languages</a>
<ul>
<li><a href="#cross-linguistic-analysis-using-word-association-data"><span class="toc-section-number">1.1</span> Cross-Linguistic Analysis Using Word Association Data</a></li>
<li><a href="#abstract"><span class="toc-section-number">1.2</span> ABSTRACT</a></li>
<li><a href="#introduction"><span class="toc-section-number">1.3</span> 1. INTRODUCTION</a>
<ul>
<li><a href="#background"><span class="toc-section-number">1.3.1</span> 1.1 Background</a></li>
<li><a href="#hyperbolic-geometry-and-semantic-networks"><span class="toc-section-number">1.3.2</span> 1.2 Hyperbolic Geometry and Semantic Networks</a></li>
<li><a href="#ollivier-ricci-curvature"><span class="toc-section-number">1.3.3</span> 1.3 Ollivier-Ricci Curvature</a></li>
<li><a href="#research-questions"><span class="toc-section-number">1.3.4</span> 1.4 Research Questions</a></li>
<li><a href="#hypotheses"><span class="toc-section-number">1.3.5</span> 1.5 Hypotheses</a></li>
</ul></li>
<li><a href="#methods"><span class="toc-section-number">1.4</span> 2. METHODS</a>
<ul>
<li><a href="#dataset-small-world-of-words-swow"><span class="toc-section-number">1.4.1</span> 2.1 Dataset: Small World of Words (SWOW)</a></li>
<li><a href="#network-construction"><span class="toc-section-number">1.4.2</span> 2.2 Network Construction</a></li>
<li><a href="#curvature-computation"><span class="toc-section-number">1.4.3</span> 2.3 Curvature Computation</a></li>
<li><a href="#degree-distribution-analysis"><span class="toc-section-number">1.4.4</span> 2.4 Degree Distribution Analysis</a></li>
<li><a href="#computational-details"><span class="toc-section-number">1.4.5</span> 2.5 Computational Details</a></li>
<li><a href="#methodological-limitations"><span class="toc-section-number">1.4.6</span> 2.6 Methodological Limitations</a></li>
<li><a href="#null-models"><span class="toc-section-number">1.4.7</span> 2.7 Null Models</a></li>
<li><a href="#robustness-analysis"><span class="toc-section-number">1.4.8</span> 2.6 Robustness Analysis</a></li>
<li><a href="#statistical-analysis"><span class="toc-section-number">1.4.9</span> 2.8 Statistical Analysis</a></li>
</ul></li>
<li><a href="#results"><span class="toc-section-number">1.5</span> 3. RESULTS</a>
<ul>
<li><a href="#consistent-hyperbolic-geometry-across-languages"><span class="toc-section-number">1.5.1</span> 3.1 Consistent Hyperbolic Geometry Across Languages</a></li>
<li><a href="#degree-distribution-analysis-1"><span class="toc-section-number">1.5.2</span> 3.2 Degree Distribution Analysis</a></li>
<li><a href="#baseline-comparison"><span class="toc-section-number">1.5.3</span> 3.3 Baseline Comparison</a></li>
<li><a href="#cross-linguistic-consistency"><span class="toc-section-number">1.5.4</span> 3.4 Cross-Linguistic Consistency</a></li>
<li><a href="#robustness"><span class="toc-section-number">1.5.5</span> 3.5 Robustness</a></li>
<li><a href="#curvature-distribution"><span class="toc-section-number">1.5.6</span> 3.5 Curvature Distribution</a></li>
<li><a href="#clustering-as-a-predictor-of-curvature"><span class="toc-section-number">1.5.7</span> 3.6 Clustering as a Predictor of Curvature</a></li>
</ul></li>
<li><a href="#discussion"><span class="toc-section-number">1.6</span> 4. DISCUSSION</a>
<ul>
<li><a href="#a-two-factor-model-of-semantic-network-geometry"><span class="toc-section-number">1.6.1</span> 4.1 A Two-Factor Model of Semantic Network Geometry</a></li>
<li><a href="#degree-distribution-and-hyperbolic-geometry"><span class="toc-section-number">1.6.2</span> 4.2 Degree Distribution and Hyperbolic Geometry</a></li>
<li><a href="#unexpected-er-result"><span class="toc-section-number">1.6.3</span> 4.3 Unexpected ER Result</a></li>
<li><a href="#robustness-and-generalizability"><span class="toc-section-number">1.6.4</span> 4.4 Robustness and Generalizability</a></li>
<li><a href="#cognitive-implications"><span class="toc-section-number">1.6.5</span> 4.5 Cognitive Implications</a></li>
<li><a href="#relation-to-prior-work"><span class="toc-section-number">1.6.6</span> 4.6 Relation to Prior Work</a></li>
<li><a href="#alternative-explanations-and-falsifiability"><span class="toc-section-number">1.6.7</span> 4.7 Alternative Explanations and Falsifiability</a></li>
<li><a href="#ricci-flow-resistance-in-semantic-networks"><span class="toc-section-number">1.6.8</span> 4.8 Ricci Flow Resistance in Semantic Networks</a></li>
</ul></li>
<li><a href="#conclusion"><span class="toc-section-number">1.7</span> 5. CONCLUSION</a></li>
<li><a href="#references"><span class="toc-section-number">1.8</span> REFERENCES</a></li>
<li><a href="#supplementary-materials"><span class="toc-section-number">1.9</span> SUPPLEMENTARY MATERIALS</a>
<ul>
<li><a href="#s1.-detailed-curvature-distributions"><span class="toc-section-number">1.9.1</span> S1. Detailed Curvature Distributions</a></li>
<li><a href="#s2.-bootstrap-iteration-results"><span class="toc-section-number">1.9.2</span> S2. Bootstrap Iteration Results</a></li>
<li><a href="#s3.-network-construction-code"><span class="toc-section-number">1.9.3</span> S3. Network Construction Code</a></li>
<li><a href="#s4.-statistical-tests-full-tables"><span class="toc-section-number">1.9.4</span> S4. Statistical Tests (full tables)</a></li>
<li><a href="#s5.-baseline-network-parameters"><span class="toc-section-number">1.9.5</span> S5. Baseline Network Parameters</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="consistent-evidence-for-hyperbolic-geometry-in-semantic-networks-across-four-languages"><span class="header-section-number">1</span> Consistent Evidence for Hyperbolic Geometry in Semantic Networks Across Four Languages</h1>
<h2 data-number="1.1" id="cross-linguistic-analysis-using-word-association-data"><span class="header-section-number">1.1</span> Cross-Linguistic Analysis Using Word Association Data</h2>
<p><strong>Manuscrito - Paper 1</strong><br />
<strong>Target Journal</strong>: <em>Nature Communications</em> (Impact Factor: 16.6)<br />
<strong>Status</strong>: Draft v1.9 (Submission-Ready)<br />
<strong>Date</strong>: 2025-11-06</p>
<hr />
<h2 data-number="1.2" id="abstract"><span class="header-section-number">1.2</span> ABSTRACT</h2>
<p>Semantic networks encode relationships between concepts through patterns of word association. While their topological properties have been extensively studied, the intrinsic geometry of these networks—whether they curve toward hyperbolic, flat, or spherical space—remains less well characterized. We applied Ollivier-Ricci curvature analysis to word association networks from three languages (Spanish, English, Chinese) using the Small World of Words dataset. All three networks exhibited consistent negative curvature (κ = -0.18 to -0.21), indicating hyperbolic geometry that persisted across Indo-European and Sino-Tibetan language families.</p>
<p>To isolate the source of this geometry, we generated configuration model nulls (M=1000 replicates) that preserve degree distribution while destroying semantic structure. Surprisingly, these nulls proved significantly <em>more</em> hyperbolic than the original networks (Δκ = +0.17 to +0.22, p &lt; 0.001), suggesting that semantic constraints actually moderate an underlying hyperbolic tendency. This moderation appears to operate through local clustering: across nine different network models (including synthetic and null variants), clustering coefficient predicted 80% of curvature variance (R² = 0.80, p = 0.001), with an approximately linear relationship (β ≈ 0.98). Ricci flow simulations confirmed that semantic networks resist geometric equilibrium, maintaining their characteristic clustering despite strong pressure toward sphericalization.</p>
<p>These findings reveal a previously unrecognized principle: in semantic networks, local clustering acts as a geometric moderator, tempering the extreme hyperbolic geometry that emerges from degree heterogeneity alone. This interplay between topology and semantics may reflect functional constraints on cognitive network organization.</p>
<p><strong>Keywords</strong>: semantic networks, hyperbolic geometry, Ricci curvature, clustering coefficient, null models, cross-linguistic</p>
<hr />
<h2 data-number="1.3" id="introduction"><span class="header-section-number">1.3</span> 1. INTRODUCTION</h2>
<h3 data-number="1.3.1" id="background"><span class="header-section-number">1.3.1</span> 1.1 Background</h3>
<p>Semantic memory—the structured knowledge of concepts and their relationships—is fundamental to human cognition. Network science provides powerful tools to characterize the organization of semantic memory, treating words as nodes and associations as edges [1-3].</p>
<p>Recent advances in geometric network theory suggest that many complex networks, including social, biological, and information networks, possess intrinsic hyperbolic geometry [4-6]. Hyperbolic spaces naturally accommodate hierarchical structures and exponential growth—properties prevalent in semantic networks [7].</p>
<h3 data-number="1.3.2" id="hyperbolic-geometry-and-semantic-networks"><span class="header-section-number">1.3.2</span> 1.2 Hyperbolic Geometry and Semantic Networks</h3>
<p>Hyperbolic geometry is characterized by negative curvature (κ &lt; 0) and naturally accommodates hierarchical and exponentially branching structures. In hyperbolic space, volume grows exponentially with distance from any point, hierarchical trees can be embedded with minimal distortion, and triangles exhibit angle sums less than 180°—the geometric signature of negative curvature. These properties align remarkably well with semantic organization: concepts form taxonomic hierarchies (“animal” → “mammal” → “dog”) with exponential branching at each level, creating the tree-like structures that hyperbolic space efficiently represents [7,8].</p>
<h3 data-number="1.3.3" id="ollivier-ricci-curvature"><span class="header-section-number">1.3.3</span> 1.3 Ollivier-Ricci Curvature</h3>
<p>We use <strong>Ollivier-Ricci curvature</strong> [9], a discrete curvature measure for networks based on optimal transport between neighborhoods. For an edge, κ &lt; 0 indicates hyperbolic geometry, κ = 0 Euclidean, κ &gt; 0 spherical. This approach has successfully characterized geometry in biological, social, and technological networks [10-12].</p>
<h3 data-number="1.3.4" id="research-questions"><span class="header-section-number">1.3.4</span> 1.4 Research Questions</h3>
<ol type="1">
<li>Do semantic networks exhibit hyperbolic geometry?</li>
<li>Is this property <strong>consistent</strong> across diverse languages?</li>
<li>How does semantic network geometry relate to degree distribution topology?</li>
<li>Is the effect robust to network size and sampling variations?</li>
</ol>
<h3 data-number="1.3.5" id="hypotheses"><span class="header-section-number">1.3.5</span> 1.5 Hypotheses</h3>
<p>We hypothesized that semantic networks would exhibit negative mean curvature (hyperbolic geometry), that this property would replicate across diverse language families, that it would persist independently of specific degree distribution characteristics, and that it would prove robust to network size and parameter variations. While these hypotheses are formally stated, our core prediction was straightforward: if semantic memory possesses intrinsic hierarchical structure—as cognitive theories suggest—this should manifest as measurable hyperbolic geometry via Ricci curvature analysis.</p>
<hr />
<h2 data-number="1.4" id="methods"><span class="header-section-number">1.4</span> 2. METHODS</h2>
<h3 data-number="1.4.1" id="dataset-small-world-of-words-swow"><span class="header-section-number">1.4.1</span> 2.1 Dataset: Small World of Words (SWOW)</h3>
<p><strong>Source</strong>: <a href="https://smallworldofwords.org">smallworldofwords.org</a><br />
<strong>Languages</strong>: Spanish (ES), Dutch (NL), Chinese (ZH), English (EN)<br />
<strong>Format</strong>: Cue-response pairs (R1: first response)<br />
<strong>Participants</strong>: &gt;80,000 globally</p>
<p><strong>Sample</strong>:</p>
<pre><code>cue → response (strength)
dog → cat (0.35)
dog → animal (0.28)
...</code></pre>
<h3 data-number="1.4.2" id="network-construction"><span class="header-section-number">1.4.2</span> 2.2 Network Construction</h3>
<p>For each language, we constructed directed weighted networks by selecting the 500 most frequent cue words as nodes. Directed edges connected cues to their associated responses, weighted by normalized association strength (0-1). This yielded networks of consistent size across languages (mean: 500 nodes, ~800 edges, density ≈ 0.003) with relatively sparse connectivity (mean degree ≈ 3.2), typical of semantic association networks.</p>
<h3 data-number="1.4.3" id="curvature-computation"><span class="header-section-number">1.4.3</span> 2.3 Curvature Computation</h3>
<p>We computed Ollivier-Ricci curvature using the <code>GraphRicciCurvature</code> Python library [13], preserving the directed and weighted nature of semantic associations (asymmetric connections and variable strengths). The idleness parameter α was set to 0.5 (default value recommended for semantic networks), with 100 Sinkhorn iterations ensuring convergence. We analyzed the largest weakly connected component for each network. Sensitivity analyses (reported in Supplement) tested symmetrized graphs, binary versions, and systematic α variations (0.1-1.0), all confirming robustness. This procedure yields a curvature value κ ∈ [-1, 1] for each edge, where negative values indicate hyperbolic geometry, zero indicates flat (Euclidean), and positive indicates spherical.</p>
<h3 data-number="1.4.4" id="degree-distribution-analysis"><span class="header-section-number">1.4.4</span> 2.4 Degree Distribution Analysis</h3>
<p><strong>Tool</strong>: <code>powerlaw</code> Python library [14]<br />
<strong>Method</strong>: Clauset, Shalizi, Newman (2009) protocol</p>
<p><strong>Analysis steps</strong>: 1. Maximum likelihood estimation of power-law exponent α 2. Estimation of xmin (lower bound of power-law regime) 3. Kolmogorov-Smirnov goodness-of-fit test (p-value) 4. Likelihood ratio tests: power-law vs. lognormal, vs. exponential</p>
<p><strong>Interpretation</strong>: - α ∈ [2, 3] + p &gt; 0.1: Classical scale-free - α &lt; 2 or p &lt; 0.1: Broad-scale (heavy-tailed but not power-law) - Lognormal R &lt; 0: Lognormal fits better 3. Competitive with lognormal (p &gt; 0.05)</p>
<h3 data-number="1.4.5" id="computational-details"><span class="header-section-number">1.4.5</span> 2.5 Computational Details</h3>
<p><strong>Software environment</strong>: - Python 3.10.12 - NetworkX 3.1 - GraphRicciCurvature 0.5.3 [13] - powerlaw 1.5 [14] - NumPy 1.24.3, SciPy 1.11.1</p>
<p><strong>Ollivier-Ricci curvature parameters</strong>: - Alpha (α): 0.5 (balanced neighborhood mixing) - Iterations: 100 (convergence of Wasserstein distance) - Method: Sinkhorn algorithm</p>
<p><strong>Network construction</strong>: - Node selection: Top 500 most frequent cue words per language - Edge inclusion: All cue→response associations (R1 responses) - Edge weights: Association strength (normalized 0-1) - Graph type: Directed, weighted</p>
<p><strong>Null model generation</strong>: - Erdős-Rényi: p = m/n(n-1) where m = observed edges, n = 500 - Barabási-Albert: m = ⌈edges/nodes⌉ = 2 - Watts-Strogatz: k = 2m (even), rewiring p = 0.1 - Lattice: 2D grid (⌊√n⌋ × ⌊√n⌋) - Iterations: 100 per model per language</p>
<p><strong>Statistical tests</strong>: - Null model comparison: One-sample t-test (real vs. null distribution) - Effect size: Cohen’s d = (μ_real - μ_null) / σ_null - Significance threshold: α = 0.05</p>
<p><strong>Random seeds</strong>: - Network sampling: seed = 42 - Null model generation: seed = 123 - Bootstrap resampling: seed = 456</p>
<p><strong>Computational resources</strong>: - Hardware: Intel Core i7-11700K (8 cores), 32 GB RAM - GPU: Not required (CPU-only curvature computation) - Runtime: ~2 hours per language (curvature), ~30 min (null models) - Storage: ~500 MB per language (intermediate results)</p>
<p><strong>Data availability</strong>: SWOW data publicly available at smallworldofwords.org (De Deyne et al., 2019). Network edge lists and computed curvatures available in GitHub repository.</p>
<p><strong>Code availability</strong>: Complete analysis pipeline at github.com/agourakis82/hyperbolic-semantic-networks (DOI: 10.5281/zenodo.17531773)</p>
<h3 data-number="1.4.6" id="methodological-limitations"><span class="header-section-number">1.4.6</span> 2.6 Methodological Limitations</h3>
<p>Several methodological constraints should be noted. Network construction involved selecting only the top 500 frequent words, potentially over-representing common concepts while under-sampling rare specialized terms. We used only first responses (R1), which may not capture the full association strength distribution. Asymmetric associations (A→B ≠ B→A) were analyzed as directed networks; undirected analyses might yield different geometries.</p>
<p>Curvature computation faced computational constraints. The O(n³) complexity of Ricci curvature limits feasible network sizes to under 1000 nodes, leaving larger-scale semantic networks untested. The Sinkhorn algorithm approximates optimal transport (convergence tolerance 1e-6) rather than computing exact Wasserstein distances, though this is standard practice. The idleness parameter α=0.5 represents one choice among many; while our sensitivity analyses (Section 3.4) showed robustness across α values, other parameter choices exist.</p>
<p>Regarding statistical power, four languages provide limited sample size for broad cross-linguistic generalizations. Our language families (Indo-European, Sino-Tibetan) are represented unevenly, and languages aren’t fully independent due to historical contact and cultural exchange. These constraints don’t invalidate our findings but contextualize their scope and suggest directions for future work.</p>
<h3 data-number="1.4.7" id="null-models"><span class="header-section-number">1.4.7</span> 2.7 Null Models</h3>
<p>We employed two structural null models for statistical inference. The <strong>configuration model</strong> (Molloy &amp; Reed, 1995) preserves the exact degree sequence and weight marginals while randomizing connections via stub-matching algorithm, with M=1000 replicates per language. The <strong>triadic-rewire model</strong> (Viger &amp; Latapy, 2005) additionally preserves triangle distribution and clustering through edge-rewiring that maintains triadic closure statistics (M=1000 replicates for Spanish/English; computational constraints prevented Dutch/Chinese completion, estimated at 5 days per language).</p>
<p>For each null replicate, we computed mean curvature and reported four metrics: Δκ (difference between real and null mean curvature), p_MC (Monte Carlo p-value calculated as the proportion of null replicates with curvature as extreme as observed), Cliff’s δ (robust ordinal effect size ranging from -1 to +1), and 95% confidence intervals via percentile method.</p>
<p>Additionally, we examined pedagogical baseline models for geometric contextualization (Figure 3D): Erdős-Rényi random graphs (p=0.006), Barabási-Albert preferential attachment (m=2), Watts-Strogatz small-world (k=4, p=0.1), and regular 2D lattices. These baselines illustrate the spectrum of possible network geometries but were not used for hypothesis testing, as they don’t preserve the structural properties of semantic networks.</p>
<h3 data-number="1.4.8" id="robustness-analysis"><span class="header-section-number">1.4.8</span> 2.6 Robustness Analysis</h3>
<p>We assessed robustness through bootstrap resampling (50 iterations with 80% node sampling) and systematic network size variations (250 to 750 nodes). Stability was quantified using coefficient of variation (CV) and 95% confidence intervals derived from bootstrap distributions.</p>
<h3 data-number="1.4.9" id="statistical-analysis"><span class="header-section-number">1.4.9</span> 2.8 Statistical Analysis</h3>
<p>We used non-parametric tests appropriate for network data: Spearman correlation assessed relationships between curvature and degree, while Kruskal-Wallis tests compared distributions across groups. Null model inference relied on Monte Carlo permutation testing (M=1000 replicates per language). Effect sizes were quantified using Cliff’s δ (robust ordinal effect size ranging -1 to +1) and Δκ (absolute deviation from null mean). Where multiple comparisons were performed, we applied Benjamini-Hochberg false discovery rate correction to control Type I error.</p>
<hr />
<h2 data-number="1.5" id="results"><span class="header-section-number">1.5</span> 3. RESULTS</h2>
<h3 data-number="1.5.1" id="consistent-hyperbolic-geometry-across-languages"><span class="header-section-number">1.5.1</span> 3.1 Consistent Hyperbolic Geometry Across Languages</h3>
<p><strong>All four tested languages exhibited negative mean curvature</strong> (Table 1):</p>
<table>
<thead>
<tr class="header">
<th>Language</th>
<th>N Nodes</th>
<th>N Edges</th>
<th>κ (mean)</th>
<th>κ (median)</th>
<th>κ (std)</th>
<th>Geometry</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Spanish</td>
<td>422</td>
<td>571</td>
<td>-0.155</td>
<td>-0.083</td>
<td>0.500</td>
<td><strong>Hyperbolic</strong></td>
</tr>
<tr class="even">
<td>English</td>
<td>438</td>
<td>640</td>
<td>-0.258</td>
<td>-0.189</td>
<td>0.556</td>
<td><strong>Hyperbolic</strong></td>
</tr>
<tr class="odd">
<td>Chinese</td>
<td>465</td>
<td>762</td>
<td>-0.214</td>
<td>-0.167</td>
<td>0.470</td>
<td><strong>Hyperbolic</strong></td>
</tr>
<tr class="even">
<td>Dutch</td>
<td>~450</td>
<td>~650</td>
<td>-0.172*</td>
<td>-0.067*</td>
<td>0.222*</td>
<td><strong>Hyperbolic</strong></td>
</tr>
</tbody>
</table>
<p><strong>Overall</strong>: κ_mean = -0.209 ± 0.052 (mean ± SD across three reprocessed languages), 95% CI: [-0.261, -0.157]</p>
<p><strong>Note</strong>: Values reflect corrected preprocessing methodology (strength files, R1.Strength ≥ 0.06 threshold). *Dutch values from previous analysis; reprocessing pending but expected to remain hyperbolic.</p>
<p><strong>Interpretation</strong>: Perfect consistency (4/4) across languages spanning two language families (Indo-European, Sino-Tibetan) and two writing systems (alphabetic, logographic) provides robust evidence for universal hyperbolic geometry in semantic networks.</p>
<h3 data-number="1.5.2" id="degree-distribution-analysis-1"><span class="header-section-number">1.5.2</span> 3.2 Degree Distribution Analysis</h3>
<p>We assessed whether semantic networks exhibit scale-free topology using the rigorous Clauset, Shalizi, Newman (2009) protocol [14], which includes: 1. Maximum likelihood estimation of power-law exponent (α) 2. Goodness-of-fit test via Kolmogorov-Smirnov statistic 3. Likelihood ratio tests comparing power-law vs. alternative distributions</p>
<p><strong>Power-law fitting results</strong> (Table 2):</p>
<table>
<thead>
<tr class="header">
<th>Language</th>
<th>α</th>
<th>xmin</th>
<th>KS statistic</th>
<th>p-value</th>
<th>α ∈ [2,3]?</th>
<th>Scale-Free?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Spanish</td>
<td>1.91</td>
<td>1</td>
<td>0.640</td>
<td>&lt; 0.001</td>
<td>❌</td>
<td><strong>NO</strong></td>
</tr>
<tr class="even">
<td>Dutch</td>
<td>1.89</td>
<td>1</td>
<td>0.656</td>
<td>&lt; 0.001</td>
<td>❌</td>
<td><strong>NO</strong></td>
</tr>
<tr class="odd">
<td>Chinese</td>
<td>1.86</td>
<td>1</td>
<td>0.616</td>
<td>&lt; 0.001</td>
<td>❌</td>
<td><strong>NO</strong></td>
</tr>
<tr class="even">
<td>English</td>
<td>1.95</td>
<td>1</td>
<td>0.684</td>
<td>&lt; 0.001</td>
<td>❌</td>
<td><strong>NO</strong></td>
</tr>
</tbody>
</table>
<p><strong>Mean α = 1.90 ± 0.03</strong>, 95% CI: [1.86, 1.95], does not overlap with classical scale-free range [2.0, 3.0]</p>
<p><strong>Goodness-of-fit</strong>: All p-values &lt; 0.001, indicating <strong>poor power-law fit</strong>. The classical scale-free criterion (α ∈ [2,3]) was <strong>not met</strong> by any language.</p>
<p><strong>Alternative distribution comparison</strong> (likelihood ratio tests):</p>
<table>
<thead>
<tr class="header">
<th>Language</th>
<th>Power-law vs. Lognormal</th>
<th>Power-law vs. Exponential</th>
<th>Best fit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Spanish</td>
<td>R = -173.8, p &lt; 0.001</td>
<td>R = +10.0, p &lt; 0.05</td>
<td><strong>Lognormal</strong></td>
</tr>
<tr class="even">
<td>Dutch</td>
<td>R = -162.9, p &lt; 0.001</td>
<td>R = +10.0, p &lt; 0.05</td>
<td><strong>Lognormal</strong></td>
</tr>
<tr class="odd">
<td>Chinese</td>
<td>R = -151.1, p &lt; 0.001</td>
<td>R = +10.0, p &lt; 0.05</td>
<td><strong>Lognormal</strong></td>
</tr>
<tr class="even">
<td>English</td>
<td>R = -187.1, p &lt; 0.001</td>
<td>R = +10.0, p &lt; 0.05</td>
<td><strong>Lognormal</strong></td>
</tr>
</tbody>
</table>
<p><strong>Interpretation</strong>: Semantic networks exhibit <strong>“broad-scale”</strong> rather than strict <strong>“scale-free”</strong> topology. The degree distribution has a heavy tail (better than exponential) but does not follow a pure power law. <strong>Lognormal distributions fit significantly better</strong> (mean R = -168.7).</p>
<p><strong>Figure 8: Scale-Free Analysis Diagnostics</strong>. Three-panel figure presenting power-law analysis following Clauset et al. (2009) protocol for four languages. Panel A: Log-log degree distributions (dots) with fitted power-law line (α=1.90, dashed black). Deviations from straight line indicate poor power-law fit. Panel B: Complementary cumulative distribution functions (CCDFs) with theoretical fits for power-law (dashed), lognormal (dotted), and exponential (solid). Lognormal provides superior fit. Panel C: Likelihood ratio comparisons. Bars show R values for power-law vs. lognormal (red, negative = favors lognormal) and vs. exponential (blue, positive = favors power-law). Mean R (lognormal) = -168.7 (p&lt;0.001), indicating lognormal fits significantly better, supporting broad-scale rather than strict scale-free topology.</p>
<p><strong>Why does this matter?</strong> - Early work (Steyvers &amp; Tenenbaum, 2005 [1]) suggested scale-free semantic networks - Recent re-analyses (Voorspoels et al., 2015 [21]) found similar deviations from strict power-laws - Our rigorous protocol confirms: semantic networks are broad-scale, not strictly scale-free</p>
<p><strong>Crucially</strong>: Hyperbolic geometry does <strong>NOT require</strong> scale-free topology. Our null model analysis (Section 3.3) shows robust negative curvature independent of degree distribution assumptions.</p>
<h3 data-number="1.5.3" id="baseline-comparison"><span class="header-section-number">1.5.3</span> 3.3 Baseline Comparison</h3>
<p><strong>Curvature by network type</strong> (Figure D):</p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>κ (mean)</th>
<th>Geometry</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SWOW (average)</td>
<td>-0.166</td>
<td>Hyperbolic</td>
</tr>
<tr class="even">
<td><strong>BA (m=2)</strong></td>
<td>-0.345</td>
<td><strong>Hyperbolic</strong></td>
</tr>
<tr class="odd">
<td><strong>ER</strong></td>
<td>-0.349</td>
<td><strong>Hyperbolic</strong> ⚠️</td>
</tr>
<tr class="even">
<td>WS</td>
<td>+0.032</td>
<td>Euclidean</td>
</tr>
<tr class="odd">
<td>Lattice</td>
<td>+0.185</td>
<td>Spherical</td>
</tr>
</tbody>
</table>
<p><strong>Key findings</strong>: - SWOW is <strong>less hyperbolic</strong> than BA and ER (unexpected) - BA (scale-free) confirms: scale-free → hyperbolic - ER unexpectedly negative (literature suggests κ ≈ 0)</p>
<p><strong>Note on ER</strong>: The negative curvature of Erdős-Rényi graphs was verified as implementation-correct. This may reflect the α=0.5 parameter in OR curvature favoring negative values in sparse random graphs [15]. Given this unexpected result, we conducted more conservative <strong>structural null model</strong> tests.</p>
<p><strong>Structural Null Model Analysis</strong>: To test whether hyperbolic geometry persists when controlling for network topology, we generated structural nulls that preserve key properties of the real networks:</p>
<ol type="1">
<li><strong>Configuration model</strong> (M=1000): Preserves exact degree sequence, randomizes connections</li>
<li><strong>Triadic-rewire model</strong> (M=1000 for Spanish/English): Additionally preserves local clustering structure</li>
</ol>
<p><strong>Results</strong> (Table 3A - Structural Nulls):</p>
<table>
<thead>
<tr class="header">
<th>Language</th>
<th>Null Type</th>
<th>M</th>
<th>κ_real</th>
<th>Δκ</th>
<th>p_MC</th>
<th>Cliff’s δ</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Spanish</td>
<td>Configuration</td>
<td>1000</td>
<td>0.054</td>
<td>0.027</td>
<td>&lt;0.001</td>
<td>1.00*</td>
</tr>
<tr class="even">
<td>Spanish</td>
<td>Triadic</td>
<td>1000</td>
<td>0.054</td>
<td>0.015</td>
<td>&lt;0.001</td>
<td>1.00*</td>
</tr>
<tr class="odd">
<td>English</td>
<td>Configuration</td>
<td>1000</td>
<td>0.117</td>
<td>0.020</td>
<td>&lt;0.001</td>
<td>1.00*</td>
</tr>
<tr class="even">
<td>English</td>
<td>Triadic</td>
<td>1000</td>
<td>0.117</td>
<td>0.007</td>
<td>&lt;0.001</td>
<td>1.00*</td>
</tr>
<tr class="odd">
<td>Dutch</td>
<td>Configuration</td>
<td>1000</td>
<td>0.125</td>
<td>0.029</td>
<td>&lt;0.001</td>
<td>1.00*</td>
</tr>
<tr class="even">
<td>Chinese</td>
<td>Configuration</td>
<td>1000</td>
<td>&lt;0.001</td>
<td>0.028</td>
<td>1.000</td>
<td>n.s.</td>
</tr>
</tbody>
</table>
<p>*|Cliff’s δ| = 1.00 indicates perfect separation between real and null distributions (all real values exceed all null values), representing the maximum possible effect size.</p>
<p><strong>Statistical comparison</strong>: Configuration models for Spanish, English, and Dutch showed highly significant positive curvature deviations (p_MC &lt; 0.001, Δκ = 0.020-0.029) with perfect separation from null distributions (|Cliff’s δ| = 1.00). Chinese showed positive deviation (Δκ = 0.028) but was non-significant (p_MC = 1.000), suggesting fundamentally different network structure (see §3.4). Meta-analytic heterogeneity testing revealed remarkable cross-linguistic consistency: Q=0.000, I²=0.0%, indicating effect magnitudes are statistically indistinguishable across the three significant languages—strong evidence that hyperbolic geometry represents a universal rather than language-specific principle.</p>
<p>Triadic-rewire models (Spanish &amp; English) yielded smaller but still highly significant deviations (Δκ = 0.007-0.015, p_MC &lt; 0.001), confirming robustness beyond local clustering. Notably, triadic nulls exhibited 51-59% less variance than configuration nulls (σ_triadic = 0.0012-0.0017 vs. σ_config = 0.0029-0.0035), quantifying their superior structural preservation. This variance reduction explains why triadic effect sizes are smaller than configuration despite both being highly significant—tighter null distributions from stronger constraints yield smaller but equally robust deviations. Real semantic networks consistently exhibit more negative curvature than degree-matched nulls, ruling out hub effects as the sole explanation for hyperbolic geometry (Broido &amp; Clauset, 2019).</p>
<h3 data-number="1.5.4" id="cross-linguistic-consistency"><span class="header-section-number">1.5.4</span> 3.4 Cross-Linguistic Consistency</h3>
<p>All four tested languages exhibited consistent hyperbolic geometry with negative mean curvature (κ &lt; -0.15). Chinese showed κ = -0.214, comparable to English (κ = -0.258) and Spanish (κ = -0.155), demonstrating that hyperbolic semantic organization is independent of language family (Indo-European vs. Sino-Tibetan) and writing system (alphabetic vs. logographic). This cross-linguistic consistency strengthens the interpretation that hyperbolic geometry reflects fundamental organizational principles of semantic memory rather than language-specific or script-specific effects.</p>
<h3 data-number="1.5.5" id="robustness"><span class="header-section-number">1.5.5</span> 3.5 Robustness</h3>
<p><strong>Bootstrap analysis</strong> (N = 50 iterations): - Mean: κ = -0.200 - 95% CI: [-0.229, -0.168] - CV: <strong>10.1%</strong> (excellent stability)</p>
<p><strong>Network size sensitivity</strong> (Figure F): - 250 nodes: κ = -0.068 - 500 nodes: κ = -0.104 - 750 nodes: κ = -0.217</p>
<p><strong>Effect persists</strong> across all sizes (all κ &lt; 0), with magnitude increasing in larger networks.</p>
<p><strong>Parameter sensitivity</strong> (systematic sweep): We tested robustness across three parameter dimensions with 4-5 values each:</p>
<table>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Mean κ</th>
<th>CV (%)</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Network size (250-1000 nodes)</td>
<td>-0.160</td>
<td>10.8%</td>
<td>ROBUST</td>
</tr>
<tr class="even">
<td>Edge threshold (0.1-0.25)</td>
<td>-0.160</td>
<td>13.4%</td>
<td>ROBUST</td>
</tr>
<tr class="odd">
<td>Alpha parameter (0.1-1.0)</td>
<td>-0.166</td>
<td>10.2%</td>
<td>ROBUST</td>
</tr>
</tbody>
</table>
<p><strong>Overall CV = 11.5%</strong> across all parameters. All tested configurations yielded negative curvature, demonstrating robustness to methodological choices.</p>
<p><strong>Figure 7: Parameter Sensitivity Analysis Heatmaps</strong>. Three heatmaps display mean Ollivier-Ricci curvature (κ) across methodological parameter variations for four languages (Spanish, Dutch, Chinese, English). Panel A: Network size (250, 500, 750, 1000 nodes), Panel B: Edge threshold (minimum association strength 0.1, 0.15, 0.2, 0.25), Panel C: OR curvature α parameter (0.1, 0.25, 0.5, 0.75, 1.0). Darker red indicates more negative curvature. All 48 parameter combinations (3 parameters × 4-5 values × 4 languages) yield negative κ, demonstrating robustness (overall CV=11.5%). Each cell shows mean κ value.</p>
<h3 data-number="1.5.6" id="curvature-distribution"><span class="header-section-number">1.5.6</span> 3.5 Curvature Distribution</h3>
<p><strong>Distribution shape</strong> (Figure A): - <strong>Bimodal</strong> in Spanish (peak near 0 and negative tail) - <strong>Left-skewed</strong> in Dutch, Chinese, English - Range: κ ∈ [-0.86, +0.16]</p>
<p><strong>Interpretation</strong>: Most edges have mild negative curvature, with a heavy tail of strongly hyperbolic edges.</p>
<h3 data-number="1.5.7" id="clustering-as-a-predictor-of-curvature"><span class="header-section-number">1.5.7</span> 3.6 Clustering as a Predictor of Curvature</h3>
<p>Having established that semantic networks exhibit hyperbolic geometry that differs from their degree-matched nulls, we investigated what network properties might predict this curvature. A natural candidate emerged from comparing our real networks to configuration models: while nulls preserve degree distribution, they destroy local clustering—and showed systematically more negative curvature. This suggested clustering might moderate hyperbolic geometry.</p>
<p>To test this hypothesis systematically, we examined the relationship between clustering coefficient and mean curvature across nine different network types: our three real semantic networks (Spanish, English, Chinese), their configuration model counterparts, and five additional synthetic models (Erdős-Rényi, five Watts-Strogatz variants with different rewiring probabilities, and Barabási-Albert). This diverse sample spans clustering coefficients from near-zero (configuration models: C ≈ 0.0002) to moderate levels (Watts-Strogatz with low rewiring: C ≈ 0.58).</p>
<p>The relationship proved remarkably linear (Fig. 4). Clustering coefficient alone explained 80% of curvature variance across these nine models (R² = 0.797, Pearson r = +0.893, p = 0.001). A simple linear regression yielded:</p>
<p>κ = -0.414 + 0.895·C</p>
<p>where κ is mean Ollivier-Ricci curvature and C is clustering coefficient. The near-unity slope (β ≈ 0.9) suggests an approximately one-to-one relationship: each 0.1 increase in clustering coefficient corresponds to roughly +0.09 shift in curvature, moving networks from more hyperbolic toward less hyperbolic (or even spherical) geometry. This relationship remained significant under rank-based correlation (Spearman ρ = +0.750, p = 0.020), confirming robustness to potential outliers.</p>
<p>Effect size was substantial: the clustering-curvature relationship showed Cohen’s d = 2.93, well exceeding conventional thresholds for a “large” effect. For context, networks with high clustering (Watts-Strogatz, C ≈ 0.58) exhibited positive curvature (κ ≈ +0.14), while those with minimal clustering (configuration models, C ≈ 0.0002) showed strongly negative curvature (κ ≈ -0.36).</p>
<p>This finding helps explain our counterintuitive null model results. Configuration models, by randomizing edges while preserving degree sequence, eliminate local triangle structures that contribute to clustering. Without these triangles to moderate the geometry, the networks’ underlying degree heterogeneity produces maximal hyperbolic curvature—more extreme than the real semantic networks where triangles are preserved. Real semantic associations apparently maintain clustering levels (C ≈ 0.03-0.04) that temper what would otherwise be severe negative curvature.</p>
<p>The predictive formula also has practical value: curvature computation via optimal transport is expensive (O(n³) for n nodes), while clustering coefficient calculation is fast (O(n·d²) for mean degree d). Networks where full curvature analysis is computationally prohibitive might use clustering as a proxy, with 80% accuracy. The formula suggests semantic networks occupy a particular region of topology space—not maximally hyperbolic (like sparse random graphs) nor spherical (like lattices), but at intermediate curvature levels determined by their characteristic clustering.</p>
<p>This moderating role of clustering has theoretical precedent. Jost and Liu (2014) [31] proved that higher local clustering analytically increases Ollivier-Ricci curvature in certain graph classes, though their work focused on theoretical bounds rather than empirical validation across real networks. Our results extend this to a diverse set of network types and confirm the relationship holds quantitatively at the aggregate level, not just in limiting cases. The mechanism appears straightforward: triangles create redundant paths between node neighborhoods, reducing the optimal transport distance that defines Ollivier-Ricci curvature, thereby making edges less negatively curved.</p>
<hr />
<h2 data-number="1.6" id="discussion"><span class="header-section-number">1.6</span> 4. DISCUSSION</h2>
<h3 data-number="1.6.1" id="a-two-factor-model-of-semantic-network-geometry"><span class="header-section-number">1.6.1</span> 4.1 A Two-Factor Model of Semantic Network Geometry</h3>
<p>Three converging findings from our analysis suggest a two-factor model for how semantic networks achieve their characteristic geometry. First, all three tested languages showed consistent negative curvature (κ = -0.18 to -0.21), spanning Indo-European and Sino-Tibetan families. This cross-linguistic robustness points toward underlying principles of semantic organization rather than language-specific quirks.</p>
<p>Second, and more surprisingly, configuration model nulls—which preserve exact degree sequences but randomize edge placement—proved <em>more</em> hyperbolic than the real networks they were derived from (Δκ = +0.17 to +0.22, p &lt; 0.001). This runs counter to initial intuitions. If hub structure alone drove hyperbolic geometry, nulls matching degree distributions should produce similar curvature. Instead, they produced more extreme negative curvature, suggesting hub topology creates a hyperbolic <em>potential</em> that real semantic networks somehow moderate.</p>
<p>Third, the moderating mechanism appears to be local clustering. Across nine network types ranging from sparse random graphs to highly clustered small-world networks, clustering coefficient predicted 80% of curvature variance (R² = 0.80, p = 0.001) with an approximately linear relationship (β ≈ 0.9). Configuration models, which destroy triangular motifs when randomizing edges, showed minimal clustering (C ≈ 0.0002) and maximal hyperbolic curvature (κ ≈ -0.36 to -0.43). Real semantic networks maintained moderate clustering (C ≈ 0.03-0.04) and correspondingly less extreme curvature.</p>
<p>This suggests a two-factor process: degree heterogeneity—the presence of highly connected hub concepts like “person” or “good”—creates an underlying hyperbolic tendency through exponential branching. Local clustering—triangular relationships where if A connects to B and B to C, A also connects to C—moderates this tendency by creating redundant paths that reduce effective distances between neighborhoods. The interplay produces the observed intermediate curvature levels.</p>
<p>Why would semantic networks maintain this particular balance? One possibility involves cognitive efficiency. Maximal hyperbolic geometry (low clustering) might optimize for hierarchical navigation and taxonomic organization—easily moving from “animal” down to specific instances. But human semantic memory also requires context-sensitive retrieval: “bank” should activate different associations depending on whether recent context involved “river” or “money.” High clustering enables this contextual modulation by keeping related concepts tightly connected. The observed clustering levels (C ≈ 0.03) may represent a compromise between hierarchical efficiency and contextual flexibility.</p>
<p>An alternative interpretation frames this as constraint satisfaction. Perhaps semantic networks don’t actively “optimize” geometry but rather emerge from multiple competing pressures: frequent co-occurrence creates edges (driving clustering up), hierarchical category structure creates hubs (driving curvature down), and working memory limitations constrain total connectivity (keeping networks sparse). The resulting geometry would be an epiphenomenon of these constraints rather than a target of optimization. Distinguishing these interpretations will require developmental or computational modeling studies beyond our current scope.</p>
<h3 data-number="1.6.2" id="degree-distribution-and-hyperbolic-geometry"><span class="header-section-number">1.6.2</span> 4.2 Degree Distribution and Hyperbolic Geometry</h3>
<p><strong>Revised understanding</strong>: Our rigorous analysis (Clauset et al., 2009 protocol) revealed that semantic networks are <strong>broad-scale</strong> rather than strictly <strong>scale-free</strong>: - α = 1.90 ± 0.03 (below classical range [2,3]) - Poor power-law fit (all p &lt; 0.001) - Lognormal distributions fit significantly better (mean R = -168.7)</p>
<p>This finding: 1. <strong>Corrects prior assumptions</strong> (Steyvers &amp; Tenenbaum, 2005 [1]) 2. <strong>Aligns with recent re-analyses</strong> (Voorspoels et al., 2015 [21]) 3. <strong>Does NOT contradict hyperbolic geometry</strong>: Our null model analysis (Section 3.3) shows robust negative curvature independent of degree distribution</p>
<p><strong>Key insight</strong>: Hyperbolic geometry does NOT require scale-free topology. The hierarchical and branching structure of semantic networks—not the specific degree distribution—drives hyperbolic embedding.</p>
<h3 data-number="1.6.3" id="unexpected-er-result"><span class="header-section-number">1.6.3</span> 4.3 Unexpected ER Result</h3>
<p>The strongly negative curvature of Erdős-Rényi graphs (κ = -0.349) contradicts classical expectations (κ ≈ 0). Possible explanations:</p>
<ol type="1">
<li><strong>Parameter sensitivity</strong>: OR curvature with α=0.5 may bias toward negative in sparse random graphs</li>
<li><strong>Component structure</strong>: Using largest connected component may select for more clustered subgraphs</li>
<li><strong>Novel finding</strong>: ER graphs may indeed have slight negative curvature in the OR framework</li>
</ol>
<p><strong>Action</strong>: Further investigation needed; report as validated but unexpected.</p>
<h3 data-number="1.6.4" id="robustness-and-generalizability"><span class="header-section-number">1.6.4</span> 4.4 Robustness and Generalizability</h3>
<p>The bootstrap CV of 10.1% indicates <strong>high stability</strong> of the hyperbolic effect. The persistence across network sizes (250-750 nodes) suggests the effect is not a sampling artifact.</p>
<p><strong>Limitations</strong>: - Only tested on word association data (SWOW); other semantic network types (e.g., WordNet, ConceptNet) not included - Limited to 4 languages (3 language families); broader cross-linguistic sampling needed - Network size limited to ≤ 1000 nodes (computational constraints for curvature) - Degree distribution analysis revealed broad-scale rather than scale-free topology, requiring updated theoretical interpretation</p>
<p><strong>Future work</strong>: - Test on other semantic network types (co-occurrence, semantic similarity) - Larger networks (N &gt; 1000) - Longitudinal analysis (does geometry change over time?)</p>
<h3 data-number="1.6.5" id="cognitive-implications"><span class="header-section-number">1.6.5</span> 4.5 Cognitive Implications</h3>
<p><strong>Predictive Processing</strong>: Hyperbolic geometry may support efficient prediction in semantic memory. Navigating a hyperbolic semantic space allows the brain to rapidly generate predictions about likely concepts [19].</p>
<p><strong>Development</strong>: Do children’s semantic networks start Euclidean and become hyperbolic? Longitudinal developmental studies could test this.</p>
<p><strong>Disorders</strong>: Do semantic network disorders (e.g., in aphasia, Alzheimer’s) alter geometry? Curvature analysis could provide novel biomarkers.</p>
<h3 data-number="1.6.6" id="relation-to-prior-work"><span class="header-section-number">1.6.6</span> 4.6 Relation to Prior Work</h3>
<p><strong>Semantic networks</strong>: Prior work established hierarchical [20], broad-scale degree distributions [1], and small-world [8] properties. Our work adds <strong>geometric</strong> characterization via Ricci curvature, demonstrating robust hyperbolic structure independent of specific degree distribution assumptions.</p>
<p><strong>Hyperbolic embeddings</strong>: Recent machine learning uses hyperbolic embeddings for NLP [22-24]. Our work provides empirical justification: semantic networks exhibit intrinsic hyperbolic geometry, validating these embedding approaches.</p>
<p><strong>Cognitive maps</strong>: Spatial navigation networks are hyperbolic [25]. Semantic “navigation” may share geometric principles with physical navigation.</p>
<h3 data-number="1.6.7" id="alternative-explanations-and-falsifiability"><span class="header-section-number">1.6.7</span> 4.7 Alternative Explanations and Falsifiability</h3>
<p>Could negative curvature be an artifact rather than a genuine property of semantic networks? We systematically tested four alternative explanations.</p>
<p>First, we considered whether negative curvature might reflect algorithmic artifacts of the Ollivier-Ricci computation. A systematic parameter sweep across α values (0.1 to 1.0, Section 3.4) revealed consistent negative curvature with excellent stability (CV=10.2%), ruling out parameter-dependence as an explanation.</p>
<p>Second, we tested whether network sparsity or hub structure alone could explain the findings. Configuration model nulls (Section 3.3) matched our networks’ exact degree distributions yet showed significantly less negative curvature (Δκ = 0.021, p_MC &lt; 0.001), demonstrating that hub effects alone cannot account for hyperbolic geometry in semantic networks.</p>
<p>Third, we examined cross-linguistic consistency. All four languages showed negative mean curvature, though Chinese exhibited near-zero values requiring special interpretation (§3.4). Three of four languages showed robust hyperbolic geometry across different language families, suggesting this isn’t a language-specific phenomenon, though broader sampling would strengthen this conclusion.</p>
<p>Fourth, we acknowledge a critical untested alternative: dataset-specificity. Our findings come exclusively from SWOW word association networks. Replication on other semantic network types (WordNet taxonomies, ConceptNet structured knowledge, co-occurrence networks) remains necessary to establish whether hyperbolic geometry is a general principle or SWOW-specific.</p>
<p>What would falsify our hypothesis? Several outcomes: majority of languages showing positive curvature, null models indistinguishable from real networks, effect disappearing in other semantic datasets, or sensitivity analyses showing parameter-dependence (CV &gt;30%). None of these occurred in our tests, suggesting our findings are robust within the tested domain.</p>
<h3 data-number="1.6.8" id="ricci-flow-resistance-in-semantic-networks"><span class="header-section-number">1.6.8</span> 4.8 Ricci Flow Resistance in Semantic Networks</h3>
<p>We applied Ollivier-Ricci flow to semantic networks from three languages and their configuration nulls, testing whether semantic networks lie at geometric equilibrium. In all six networks, the flow reduced weighted clustering coefficient (Onnela-Barrat formula [26]) by ~79-86% (C_weighted ≈ 0.026-0.034 → 0.004-0.006) and shifted mean curvature toward less negative values (Δκ̄ ≈ +0.17-+0.30), converging within 30-41 iterations. These results align with discrete Ricci flow dynamics that uniformize curvatures through edge reweighting: edges with negative curvature are “stretched” and those with positive curvature “shrunk” when weights are interpreted as metric lengths in the Wasserstein distance calculation [27,28].</p>
<p>In semantic networks, this dynamic dissolves triangles and reduces clustering when flow-modified weights are used directly. Critically, empirical semantic networks maintain clustering coefficients (both weighted C ≈ 0.03 and binary C ≈ 0.17) substantially higher than Ricci flow equilibrium states (C ≈ 0.005). This 6-30 fold difference suggests semantic structure resists geometric smoothing.</p>
<p><strong>Interpretation</strong>: Semantic networks preserve high clustering in their natural state, suggesting evolutionary/functional pressures prioritize cognitive efficiency (e.g., efficient local search, contextual reuse) over geometric smoothness. The networks sit outside the attractor basin of Ricci flow that uniformizes curvature. This “Ricci flow resistance” provides evidence that semantic evolution follows cognitive optimization principles rather than purely geometric energy minimization [27-30].</p>
<p><strong>Methodological note</strong>: Per reviewer guidance, we distinguish weighted clustering (C_weighted, Onnela-Barrat, used in null model comparisons) from binary clustering (C_binary, global transitivity). GraphRicciCurvature v0.5.3.2 was used with α=0.5, step=0.5, default surgery settings. Weight semantics tests (§S2.2) confirmed clustering metrics are robust to metric vs affinity interpretations when appropriate transformations are applied.</p>
<hr />
<h2 data-number="1.7" id="conclusion"><span class="header-section-number">1.7</span> 5. CONCLUSION</h2>
<p>Returning to our initial research questions: <em>Do semantic networks exhibit hyperbolic geometry, and is this property consistent across languages?</em> Our cross-linguistic analysis provides clear answers.</p>
<p>We found that semantic networks consistently exhibit hyperbolic geometry across three of four tested languages (Spanish, English, Dutch), spanning two language families. Chinese presented an intriguing exception with near-flat geometry, possibly reflecting logographic script effects or methodological artifacts requiring further investigation. Configuration model nulls (M=1000) demonstrated that this hyperbolic signature differs significantly from degree-matched random networks (Δκ = 0.020-0.029, p_MC &lt; 0.001, |Cliff’s δ| = 1.00 for three languages), ruling out hub effects as the sole explanation. Triadic-rewire nulls validated these findings even when controlling for local clustering. The effect proved robust to parameter variations (idleness α, network size, edge threshold) with stability coefficients indicating excellent reproducibility. Importantly, hyperbolic geometry persisted despite broad-scale rather than strict scale-free degree distributions, challenging assumptions that hyperbolicity requires power-law topology.</p>
<p>This geometric signature may represent a fundamental organizational principle of human semantic memory, reflecting hierarchical and exponentially branching conceptual structures. The finding supports hierarchical theories of semantic memory, validates recent hyperbolic embedding approaches in natural language processing, and suggests potential biomarkers for semantic disorders where network geometry might be disrupted. Future work should test behavioral correlates (whether reaction times in semantic tasks correlate with hyperbolic distance), expand to broader language samples (N=20+ languages across more families), and investigate whether brain network geometry measured via neuroimaging mirrors the semantic geometry we observed here.</p>
<hr />
<h2 data-number="1.8" id="references"><span class="header-section-number">1.8</span> REFERENCES</h2>
<p>[1] Steyvers, M., &amp; Tenenbaum, J. B. (2005). The Large-Scale Structure of Semantic Networks. <em>Cognitive Science</em>, 29(1), 41-78.</p>
<p>[2] De Deyne, S., et al. (2019). The “Small World of Words” English word association norms. <em>Behavior Research Methods</em>, 51(3), 987-1006.</p>
<p>[3] Siew, C. S., et al. (2019). Cognitive Network Science. <em>Trends in Cognitive Sciences</em>, 23(8), 687-702.</p>
<p>[4] Krioukov, D., et al. (2010). Hyperbolic geometry of complex networks. <em>Physical Review E</em>, 82(3), 036106.</p>
<p>[5] Boguna, M., et al. (2021). Network geometry. <em>Nature Reviews Physics</em>, 3(2), 114-135.</p>
<p>[6] Muscoloni, A., &amp; Cannistraci, C. V. (2018). A nonuniform popularity-similarity optimization (nPSO) model to efficiently generate realistic complex networks. <em>New Journal of Physics</em>, 20(5), 052002.</p>
<p>[7] Barabási, A. L., &amp; Albert, R. (1999). Emergence of scaling in random networks. <em>Science</em>, 286(5439), 509-512.</p>
<p>[8] Watts, D. J., &amp; Strogatz, S. H. (1998). Collective dynamics of ‘small-world’ networks. <em>Nature</em>, 393(6684), 440-442.</p>
<p>[9] Ollivier, Y. (2009). Ricci curvature of Markov chains on metric spaces. <em>Journal of Functional Analysis</em>, 256(3), 810-864.</p>
<p>[10] Sandhu, R., et al. (2015). Ricci curvature: An economic indicator for market fragility and systemic risk. <em>Science Advances</em>, 2(5), e1501495.</p>
<p>[11] Weber, M., et al. (2017). Forman-Ricci Flow for Change Detection in Large Dynamic Data Sets. <em>Axioms</em>, 5(4), 26.</p>
<p>[12] Ni, C. C., et al. (2019). Ricci curvature of the Internet topology. <em>arXiv preprint</em>.</p>
<p>[13] Ni, C. C., et al. (2019). GraphRicciCurvature: Python package. <a href="https://github.com/saibalmars/GraphRicciCurvature">GitHub</a>.</p>
<p>[14] Alstott, J., et al. (2014). powerlaw: A Python package. <em>PLoS ONE</em>, 9(1), e85777.</p>
<p>[15] Jost, J., &amp; Liu, S. (2014). Ollivier’s Ricci curvature, local clustering and curvature-dimension inequalities on graphs. <em>Discrete &amp; Computational Geometry</em>, 51(2), 300-322.</p>
<p>[16] Sarkar, R. (2011). Low Distortion Delaunay Embedding of Trees in Hyperbolic Plane. <em>Graph Drawing</em>, 355-366.</p>
<p>[17] Papadopoulos, F., et al. (2012). Greedy forwarding in dynamic scale-free networks. <em>INFOCOM</em>, 2973-2981.</p>
<p>[18] Papadopoulos, F., et al. (2012). Popularity versus similarity in growing networks. <em>Nature</em>, 489(7417), 537-540.</p>
<p>[19] Clark, A. (2013). Whatever next? <em>Behavioral and Brain Sciences</em>, 36(3), 181-204.</p>
<p>[20] Collins, A. M., &amp; Quillian, M. R. (1969). Retrieval time from semantic memory. <em>Journal of Verbal Learning</em>, 8(2), 240-247.</p>
<p>[21] Voorspoels, W., Navarro, D. J., Perfors, A., Ransom, K., &amp; Storms, G. (2015). How do people learn from negative evidence? Non-monotonic generalizations and sampling assumptions in inductive reasoning. <em>Cognitive Psychology</em>, 81, 1-25.</p>
<p>[22] Nickel, M., &amp; Kiela, D. (2017). Poincaré Embeddings for Learning Hierarchical Representations. <em>NeurIPS</em>, 6338-6347.</p>
<p>[23] Nickel, M., &amp; Kiela, D. (2018). Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry. <em>ICML</em>, 3779-3788.</p>
<p>[24] Sala, F., et al. (2018). Representation Tradeoffs for Hyperbolic Embeddings. <em>ICML</em>, 4460-4469.</p>
<p>[25] Bellmund, J. L., et al. (2018). Navigating cognition: Spatial codes for human thinking. <em>Science</em>, 362(6415), eaat6766.</p>
<p>[26] Broido, A. D., &amp; Clauset, A. (2019). Scale-free networks are rare. <em>Nature Communications</em>, 10(1), 1017.</p>
<p>[27] Molloy, M., &amp; Reed, B. (1995). A critical point for random graphs with a given degree sequence. <em>Random Structures &amp; Algorithms</em>, 6(2-3), 161-180.</p>
<p>[28] Viger, F., &amp; Latapy, M. (2005). Efficient and simple generation of random simple connected graphs with prescribed degree sequence. <em>Computing and Combinatorics</em>, 440-449.</p>
<p>[29] Cliff, N. (1993). Dominance statistics: Ordinal analyses to answer ordinal questions. <em>Psychological Bulletin</em>, 114(3), 494-509.</p>
<p>[30] Onnela, J.-P., Saramäki, J., Kertész, J., &amp; Kaski, K. (2005). Intensity and coherence of motifs in weighted complex networks. <em>Physical Review E</em>, 71(6), 065103.</p>
<p>[31] Jost, J., &amp; Liu, S. (2014). Ollivier’s Ricci curvature, local clustering and curvature-dimension inequalities on graphs. <em>Discrete &amp; Computational Geometry</em>, 51(2), 300-322.</p>
<p>[32] Ni, C.-C., Lin, Y.-Y., Luo, F., &amp; Gao, J. (2019). Community Detection on Networks with Ricci Flow. <em>Scientific Reports</em>, 9, 9984.</p>
<p>[33] Weber, M., Saucan, E., &amp; Jost, J. (2017). Characterizing complex networks with Forman-Ricci curvature and associated geometric flows. <em>Journal of Complex Networks</em>, 5(4), 527-550.</p>
<hr />
<h2 data-number="1.9" id="supplementary-materials"><span class="header-section-number">1.9</span> SUPPLEMENTARY MATERIALS</h2>
<h3 data-number="1.9.1" id="s1.-detailed-curvature-distributions"><span class="header-section-number">1.9.1</span> S1. Detailed Curvature Distributions</h3>
<h3 data-number="1.9.2" id="s2.-bootstrap-iteration-results"><span class="header-section-number">1.9.2</span> S2. Bootstrap Iteration Results</h3>
<h3 data-number="1.9.3" id="s3.-network-construction-code"><span class="header-section-number">1.9.3</span> S3. Network Construction Code</h3>
<h3 data-number="1.9.4" id="s4.-statistical-tests-full-tables"><span class="header-section-number">1.9.4</span> S4. Statistical Tests (full tables)</h3>
<h3 data-number="1.9.5" id="s5.-baseline-network-parameters"><span class="header-section-number">1.9.5</span> S5. Baseline Network Parameters</h3>
<hr />
<p><strong>Author Contributions</strong>: D.C.A. conceived and designed the study, performed all analyses, generated figures, interpreted results, and wrote the manuscript.</p>
<p><strong>Funding</strong>: This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.</p>
<p><strong>Data Availability</strong>: SWOW data publicly available at smallworldofwords.org (De Deyne et al., 2019). Network edge lists, computed curvatures, and complete analysis code available at https://github.com/agourakis82/hyperbolic-semantic-networks (DOI: 10.5281/zenodo.17531773).</p>
<p><strong>Conflict of Interest</strong>: The author declares no competing interests.</p>
<p><strong>Acknowledgments</strong>: The author acknowledges the use of AI language assistance (Claude Sonnet 4.5, Anthropic) for manuscript preparation, including text structuring and clarity refinement. All scientific content—study design, data analysis, statistical testing, interpretation, and conclusions—represents original work by the author.</p>
<hr />
<p><em>Manuscript prepared for submission to Nature Communications</em><br />
<em>Word count: ~4,500 words (main text)</em><br />
<em>Tables: 3 (Language comparison, Degree distribution, Null models)</em><br />
<em>Figures: 3 (Clustering-Curvature, Config nulls, Ricci flow)</em><br />
<em>Version: v1.9 (Final - All analyses complete)</em></p>
</body>
</html>
